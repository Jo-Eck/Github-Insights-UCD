{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "* First, we need to import the necessary libraries.\n",
    "* Then we load the configuration file.\n",
    "  * The config file contains information like the database config and the API key\n",
    "* Then we setup the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "import configparser as cp\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Reads the config file and returns the config object\n",
    "\"\"\"\n",
    "config = cp.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite\n",
    "\n",
    "I decided to user a SQLite database instead of simply keeping the data in memory or in a JSON file\\nnot just because of the size of the dataset, but also because of the inherent structure of the data itself.\n",
    "\n",
    "As it is to be expected that the the n to n relationship between the Repos and the Contributors will be queried a lot, so it makes more sense to have a database which can handle relational requests, instead of manually joining dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(config['DB']['NAME'])\n",
    "   \n",
    "\n",
    "db.execute(\"\"\"\n",
    "           CREATE TABLE IF NOT EXISTS users (\n",
    "               id TEXT PRIMARY KEY,\n",
    "               name TEXT NOT NULL,\n",
    "               location TEXT,\n",
    "               addedToDB datetime,\n",
    "               createdAt datetime\n",
    "            )\n",
    "              \"\"\")\n",
    "\n",
    "db.execute(\"\"\" \n",
    "           CREATE TABLE IF NOT EXISTS repos (\n",
    "               id TEXT PRIMARY KEY,\n",
    "               name TEXT NOT NULL,\n",
    "               owner TEXT,\n",
    "               url TEXT,\n",
    "               stargazerCount INTEGER,\n",
    "               watchers INTEGER,\n",
    "               primaryLanguage text,\n",
    "               isFork boolean,\n",
    "               forkCount INTEGER,\n",
    "               updatedAt datetime,\n",
    "               createdAt datetime,\n",
    "               addedToDB datetime,\n",
    "               allCommits boolean,\n",
    "               FOREIGN KEY (owner) REFERENCES users(id)\n",
    "            )\n",
    "           \"\"\")\n",
    "\n",
    "db.execute(\"\"\"\n",
    "           CREATE TABLE IF NOT EXISTS commits (\n",
    "               id TEXT PRIMARY KEY,\n",
    "               repo TEXT NOT NULL,\n",
    "               user TEXT,\n",
    "               createdAt datetime,\n",
    "               additions INTEGER,\n",
    "               deletions INTEGER,\n",
    "               addedToDB datetime,\n",
    "               FOREIGN KEY (repo) REFERENCES repos(id),\n",
    "               FOREIGN KEY (user) REFERENCES users(id)\n",
    "            )\n",
    "            \"\"\")\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to insert a repo into the database\n",
    "INSERT_REPO = \"\"\"\n",
    "                        INSERT OR IGNORE INTO repos(\n",
    "                            id,\n",
    "                            name,\n",
    "                            owner,\n",
    "                            url,\n",
    "                            stargazerCount,\n",
    "                            watchers,\n",
    "                            %s\n",
    "                            isFork,\n",
    "                            forkCount,\n",
    "                            updatedAt,\n",
    "                            createdAt,\n",
    "                            addedToDB,\n",
    "                            allCommits\n",
    "                        )\n",
    "                        VALUES(\"%s\", \"%s\", \"%s\",\"%s\",  %s,  %s, %s %s, %s, %s, %s, CURRENT_TIMESTAMP, FALSE)\n",
    "                        \"\"\"\n",
    "\n",
    "# Query to insert a user into the database\n",
    "INSERT_USER = \"\"\"\n",
    "                        INSERT OR IGNORE INTO users(id, name %s %s, addedToDB)\n",
    "                        VALUES( \"%s\", \"%s\" %s %s, CURRENT_TIMESTAMP)\n",
    "                    \"\"\"\n",
    "\n",
    "# Query to insert a commit into the database\n",
    "INSERT_COMMIT = \"\"\"\n",
    "                          INSERT OR IGNORE INTO commits(id, repo, user, createdAt, additions, deletions, addedToDB)\n",
    "                          VALUES(\"%s\", \"%s\", \"%s\", %s, %s, %s, CURRENT_TIMESTAMP)\n",
    "                      \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting up the Queries\n",
    "\n",
    "Due to Githubs limitation on the number of 1000 items returned per query we need to create queries which get less than 1000 items, but still cover the entirety of the dataset.\n",
    "\n",
    "Previous attempts\\[1\\] to solve this exact problem constrained their queries by the amount of stars for each repository. \\\n",
    "A method, which only works as long a there are less than 1000 repositories with the same amount of stars.\n",
    "\n",
    "This was then mitigated by using the creation date of the repository as a second constraint.\\\n",
    "As described in their corresponding blog article \\[2\\], this solution works by:\n",
    "\n",
    "* First querying the Github Graphql API to see the result count of how many items a given query would provide\n",
    "* If it is above a count of 1000 results the takes the date of jungest and oldest repository and splits the query in half of the time range\n",
    "* Then the size of these two queries is checked again and if they are still above 1000 results the process is repeated until the size of the queries is below 1000 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to convert a Unix timestamp to a string in the format required by the github api\n",
    "to_string = lambda stamp : datetime.fromtimestamp(stamp).strftime('%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = {}\n",
    "\n",
    "start = 1167609600 # Timestamp for 2007-01-01 (Github was founded in 2008 so this will cover all repos)\n",
    "end = 1678209714  # Current Time stamp (for consistency will not use time.time()\n",
    "\n",
    "amount_of_repos = None\n",
    "\n",
    "# The query to get the number of repos in a given time range as well as the current state of the rate limit\n",
    "count_query = ''' query { \n",
    "                   rateLimit {\n",
    "                    cost\n",
    "                    remaining\n",
    "                    resetAt\n",
    "                  }\n",
    "                  search(\n",
    "                    query:\"is:public, stars:>%s, created:%s..%s\"\n",
    "                    type: REPOSITORY, first: 1) {\n",
    "                    repositoryCount\n",
    "                  }\n",
    "                } '''\n",
    "\n",
    "\n",
    "def split_querys(start, end):\n",
    "  global amount_of_repos\n",
    "  global repos_done\n",
    "\n",
    "  repo_count_response = requests.post(\n",
    "          'https://api.github.com/graphql',\n",
    "          headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "          json={\"query\": count_query % (config['GENERAL']['STARS'], to_string(start), to_string(end))}\n",
    "      )\n",
    "  \n",
    "  # On the first run we get the total number of repos \n",
    "  # This is used to calculate the progress of the script\n",
    "  if (amount_of_repos is None):\n",
    "    amount_of_repos = repo_count_response.json()[\"data\"][\"search\"][\"repositoryCount\"]\n",
    "    sections[\"amount_of_repos\"]=  amount_of_repos\n",
    "    repos_done = 0\n",
    "\n",
    "  # If we are close to the rate limit we sleep until the rate limit resets\n",
    "  if repo_count_response.json()[\"data\"][\"rateLimit\"][\"remaining\"] < 10:\n",
    "    reset_time = datetime.strptime( repo_count_response.json()[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    while datetime.now() < reset_time:\n",
    "      seconds_till_reset = (reset_time - datetime.now()).total_seconds()\n",
    "      print (\"Sleeping till %s... %d minutes and %d seconds left...\" % ( reset_time, *divmod(seconds_till_reset, 60)))\n",
    "      time.sleep(5)\n",
    "\n",
    "  # If the number of repos in the repos in the time range is greater than 1000\n",
    "  if repo_count_response.json()[\"data\"][\"search\"][\"repositoryCount\"] > 1000:\n",
    "    # We split the range in half and do the same query on each half\n",
    "    # This will continue recursively until the number of repos is less than 1000\n",
    "    split_querys(start, (start + end)//2)\n",
    "    split_querys((start + end)//2, end) \n",
    "    \n",
    "  else:\n",
    "    # If we finnaly get a range with less than 1000 repos we add the timestamps to the sections list\n",
    "    sections.update({f\"{start}-{end}\": {\"start\": start, \"end\": end, \"inDb\": False}})\n",
    "    repos_done = repos_done+repo_count_response.json()[\"data\"][\"search\"][\"repositoryCount\"]\n",
    "    print(f\"Working on {to_string(start)} to {to_string(end)}. Progress: {repos_done/amount_of_repos*100:.2f}% Status:{repo_count_response.status_code}\")\n",
    "\n",
    "if os.path.exists(config[\"FILES\"][\"SECTIONS\"]):\n",
    "  with open(config[\"FILES\"][\"SECTIONS\"], 'r') as f:\n",
    "    sections = json.loads(f.read())\n",
    "else:\n",
    "  split_querys(start, end)\n",
    "  with open(config[\"FILES\"][\"SECTIONS\"], 'w') as f:\n",
    "      f.write(json.dumps(sections, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the data to the database\n",
    "\n",
    "While we are querying the API, we are also need a way to store the data we are getting.\n",
    "It doen't make sense to store the entire data in memory to serialize it later on, \\\n",
    "because the size of the data would be too large and if we where to run into an error we cant recover from (like a longer than anticipated connection issue), \\\n",
    "we would lose all the data we have already queried.\n",
    "\n",
    "Therefore we are going to store the data in the previously setup SQLite database, \\\n",
    "by preparing the insert statements now and executing them as we get the data from the API. \\\n",
    "This is done by adding each repository to the database and assigning it a creator.  \\\n",
    "If that creator doesn't not already exist in the database, we add it as well.\n",
    "\n",
    "The data we recieve does not always contain the same information, so we need to check if the data is present and if not we add a null value or an empty string to the database.\\\n",
    "The empty strings can later be cleaned or replaced as needed, during the data analysis.\n",
    "\n",
    "The upper part show the code for adding the data to the database once received from the API in form of a json object. \\\n",
    "Below that ist the actually query we send to the api, due to it being a GraphQL query it looks \\\n",
    "a bit different than a normal REST API query.\n",
    "\n",
    "\n",
    "For logging purposes we also add the date of the query to the database, so we can see how the data changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_repos_to_db(nodes):\n",
    "    for node in nodes:\n",
    "        item = node[\"node\"]\n",
    "        db.execute(INSERT_USER % (\n",
    "                                \", createdAt\" if item[\"owner\"].get(\"createdAt\") else \"\",\n",
    "                                \", location\" if item[\"owner\"].get(\"location\") else \"\",\n",
    "                                item[\"owner\"][\"id\"],\n",
    "                                item[\"owner\"].get(\"login\") if item[\"owner\"].get(\"login\") else \"\",\n",
    "                                f\", \\n {datetime.strptime(item['owner']['createdAt'], '%Y-%m-%dT%H:%M:%SZ').timestamp()}\" if item[\"owner\"].get(\"createdAt\") else \"\",\n",
    "                                # The following line is nasty but will do for now #TODO fix this\n",
    "                                \", \\n '%s'\" % (item['owner']['location'].replace(\"'\", r\"''\")) if item[\"owner\"].get(\"location\") else \"\",\n",
    "                            )\n",
    "                    )\n",
    "        db.execute(INSERT_REPO % (\n",
    "                                    f\"primaryLanguage,\" if item.get(\"primaryLanguage\") else \"\",\n",
    "                                    item[\"id\"],\n",
    "                                    item[\"name\"],\n",
    "                                    item[\"owner\"][\"id\"],\n",
    "                                    item[\"url\"],\n",
    "                                    item[\"stargazerCount\"],\n",
    "                                    item[\"watchers\"][\"totalCount\"],\n",
    "                                    f'\"{item[\"primaryLanguage\"][\"name\"]}\",' if item.get(\"primaryLanguage\") else \"\",\n",
    "                                    item[\"isFork\"],\n",
    "                                    item[\"forkCount\"],\n",
    "                                    datetime.strptime(item[\"updatedAt\"], '%Y-%m-%dT%H:%M:%SZ').timestamp(),\n",
    "                                    datetime.strptime(item[\"createdAt\"], '%Y-%m-%dT%H:%M:%SZ').timestamp()\n",
    "                                )\n",
    "                    )\n",
    "    db.commit()\n",
    "\n",
    "\n",
    "repo_query= \"\"\"\n",
    "              {\n",
    "                rateLimit {\n",
    "                  cost\n",
    "                  remaining\n",
    "                  resetAt\n",
    "                }\n",
    "                search(\n",
    "                  query: \"is:public, stars:>%s, created:%s..%s\"\n",
    "                  %s\n",
    "                  type: REPOSITORY\n",
    "                  first: 100\n",
    "                ) {\n",
    "                  repositoryCount\n",
    "                  pageInfo {\n",
    "                    hasNextPage\n",
    "                    endCursor\n",
    "                  }\n",
    "                  edges {\n",
    "                    node {\n",
    "                      ... on Repository {\n",
    "                        createdAt\n",
    "                        forkCount\n",
    "                        isFork\n",
    "                        updatedAt\n",
    "                        primaryLanguage {\n",
    "                          name\n",
    "                        }\n",
    "                        watchers {\n",
    "                          totalCount\n",
    "                        }\n",
    "                        stargazerCount\n",
    "                        databaseId\n",
    "                        owner {\n",
    "                          id\n",
    "                          ... on User {\n",
    "                            id\n",
    "                            createdAt\n",
    "                            location\n",
    "                            databaseId\n",
    "                            login\n",
    "                          }\n",
    "                        }\n",
    "                        id\n",
    "                        name\n",
    "                        url\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Github API\n",
    "\n",
    "Now that we have can query the github with sizeable chunks of data, we can start to query the API.\n",
    "We are still using the graphql API for this, as it enables us to fetch only the data we actually need.\n",
    "The REST API would require us to fetch the entire repository object, which contains a lot of unnecessary and redundant data.\n",
    "\n",
    "Just because we are now able to query the bite sized chunks of data, doesn't mean that the query will actually return them.\n",
    "In order to keep the loading times of the website low, Github uses pagination to limit the amount of data returned per query.\n",
    "This means that we can only get 100 items per query, which is why we need to use the `endCursor` to get the next 100 items.\n",
    "\n",
    "The cursor functions like a little bookmark, which tells the API where we left off and where to continue from, it needs to be passed as a parameter to the next query.\n",
    "\n",
    "### Querying the Repos\n",
    "\n",
    "The query itself consist of 4 parts:\n",
    "\n",
    "1. A little snippet, requesting the current state of the rate limit, so we can keep track of how many requests we have left and when to stop\n",
    "2. The filter for the repositories consisting of the following:\n",
    "    * Only repositories which are public (this is a bit redundant, as the API only returns public repositories or the ones you have access to)\n",
    "    * A limit on the amount of stars the repository has, everything below 15 is being ignored as it indicates little relevance\n",
    "    * The date range of the repositories, this is where we plug in our previously calculated date ranges\n",
    "3. Then we request a little bit more metadata about the query itself, like the total count of items and the cursor for the next page and whether there is a next page at all\n",
    "4. Then we tell the API exactly what kind of values we are interested in\n",
    "   1. This being information about the repository itself, like the name, the url, the description, the creation date, the amount of stars and the amount of forks\n",
    "   2. but also information about its creator, like the name, the profile creation date and its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_downloaded = 0 # Used to keep track of the progress of the script\n",
    "\n",
    "with open(config[\"FILES\"][\"SECTIONS\"]) as json_file:\n",
    "  file = json.load(json_file)\n",
    "  amount_of_repos = file[\"amount_of_repos\"] \n",
    "  sections = file[\"sections\"]\n",
    "\n",
    "\n",
    "for index in [x for x in sections if sections[x][\"inDb\"] == False]:\n",
    "  section = sections[index]\n",
    "\n",
    "  print(\"=\" * 100)\n",
    "  print(f\"Downloading repos from {to_string(section['start'])} to {to_string(section['end'])}\")\n",
    "\n",
    "  \n",
    "  cursor = None # Used to keep track of the current page in the query\n",
    "  has_next_page = True # Used to indicate if there are more pages to query\n",
    "\n",
    "  \n",
    "  while (has_next_page):\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    repo_query_response = requests.post(\n",
    "              'https://api.github.com/graphql',\n",
    "              headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "              json={\"query\": repo_query % (\n",
    "                config['GENERAL'][\"STARS\"],\n",
    "                to_string(section[\"start\"]),\n",
    "                to_string(section[\"end\"]),\n",
    "                f\"after: \\\"{cursor}\\\"\" if cursor else \"\"\n",
    "              )}\n",
    "          )\n",
    "    \n",
    "    # If we are close to the rate limit we sleep until the rate limit resets\n",
    "    if repo_query_response.json()[\"data\"][\"rateLimit\"][\"remaining\"] < 10:\n",
    "      reset_time = datetime.strptime( repo_query_response.json()[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "      while datetime.now() < reset_time:\n",
    "        seconds_till_reset = (reset_time - datetime.now()).total_seconds()\n",
    "        print (\"Sleeping till %s... %d minutes and %d seconds left...\" % ( reset_time, *divmod(seconds_till_reset, 60)))\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Summing up the progress made so far\n",
    "    repos_downloaded = repos_downloaded + len(repo_query_response.json()[\"data\"][\"search\"][\"edges\"])\n",
    "    \n",
    "    # Updating the cursor and has_next_page variables to know if and where to continue the query\n",
    "    cursor = repo_query_response.json()[\"data\"][\"search\"][\"pageInfo\"][\"endCursor\"]\n",
    "    has_next_page = repo_query_response.json()[\"data\"][\"search\"][\"pageInfo\"][\"hasNextPage\"]\n",
    "    \n",
    "   \n",
    "    add_repos_to_db(repo_query_response.json()[\"data\"][\"search\"][\"edges\"])\n",
    "    section[\"inDb\"] = True\n",
    "    \n",
    "    file[\"sections\"] = sections\n",
    "    with open('sections.json', 'w+') as outfile:\n",
    "      outfile.write(json.dumps(file, indent=4))\n",
    "\n",
    "    # Presenting the progress of the script\n",
    "    print(f\"\"\"Downloaded {repos_downloaded}/{amount_of_repos} repos \\\n",
    "          Requests left: {repo_query_response.json()['data']['rateLimit']['remaining']} \\\n",
    "          Progress: {repos_downloaded/amount_of_repos*100:.2f}%.\"\"\")\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries for the Commits\n",
    "\n",
    "But we do not only want the data about the repositories and the person who initially created them, we also want to know who actually worked on them. \\\n",
    "This is where the commits come in...\n",
    "\n",
    "The commits are the actual changes to the code, which are made by contributors. \\\n",
    "We could have queried this during the repository query, but this seems to overload the API and also would make the code utterly unreadable.\n",
    "\n",
    "This is why we simply query the commits for every repository separately, which makes the query itself a lot simpler.\n",
    "\n",
    "To get to the commits we need to use the creator name and the repository name, as the api does not accept the repository id itself. \\\n",
    "Unfortunately the creator name is not always provided by our query, this is due to the fact, that not only users can create repositories, but also organizations. \\\n",
    "If a repository is created by an organization, the creator name is not provided by the API call directly.\n",
    "\n",
    "Indirectly we can get the creator name by spitting up the repository name by the `/` and taking the first part of the string, which is the name of the organization.\\\n",
    "With this information we can then query the API for the commits.\n",
    "\n",
    "The structurally the query is very similar to the one for the repositories, we are still using the graphql API and we are still traversing.\n",
    "\n",
    "#### Parallelizing the queries\n",
    "\n",
    "Querying the 1.236.664 repositories took several hours. Which is due to the fact, that we didn't start to query the the next repo until we got the response for the previous one, doing it that way is called sequential querying or synchronous querying.\n",
    "It takes a lot of time but I just left it running over night and it was done in the morning. :)\n",
    "\n",
    "But now every repo having X commits on average, resulting in X queries for every repo, doing it the simple synchronous would result in a lot of waiting time....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_query = \"\"\"\n",
    "                 {\n",
    "                  rateLimit {\n",
    "                    cost\n",
    "                    remaining\n",
    "                    resetAt\n",
    "                  }\n",
    "                  repository(name: \"%s\", owner: \"%s\") {\n",
    "                    id,\n",
    "                    defaultBranchRef {\n",
    "                      target {\n",
    "                        ... on Commit {\n",
    "                          id\n",
    "                          history(first: 100 %s) {\n",
    "                            edges {\n",
    "                              node {\n",
    "                                id\n",
    "                                committedDate\n",
    "                                additions\n",
    "                                deletions   \n",
    "                                author {\n",
    "                                  user {\n",
    "                                    id\n",
    "                                    login\n",
    "                                    location\n",
    "                                  }\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                            totalCount\n",
    "                            pageInfo {\n",
    "                              endCursor\n",
    "                              hasNextPage\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\"\"\"\n",
    "                \n",
    "wait = 0\n",
    "seconday_rate_limit_message = 'You have exceeded a secondary rate limit. Please wait a few minutes before you try again.'\n",
    "\n",
    "async def get_commits(repo):\n",
    "  \"\"\"\n",
    "  This function is used to query the GitHub API for commits of a given repository.\n",
    "  \n",
    "  repo: A list containing the owner and name of the repository\n",
    "  \"\"\"\n",
    "  \n",
    "  page = 0 \n",
    "  global wait\n",
    "  print(f\"Getting commits for page {page} of {repo[0]}/{repo[1]}\")\n",
    "  cursor = None # Used to keep track of the current page in the query\n",
    "  has_next_page = True # Used to indicate if there are more pages to query\n",
    "\n",
    "  while (has_next_page):\n",
    "    \n",
    "    while time.time() < wait:\n",
    "      wait_time = wait - time.time() + random.randint(1, 10)*0.01\n",
    "      time.sleep(wait_time)\n",
    "    \n",
    "    # This part is where the actual query is made to the GitHub API\n",
    "    # It is wrapped in a try except block to catch any connection errors\n",
    "    # If a connection error occurs the script will try again at a later time\n",
    "    \n",
    "    try:\n",
    "      async with aiohttp.ClientSession() as session:\n",
    "          async with session.post('https://api.github.com/graphql',\n",
    "                  headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "                  json={\"query\": commit_query % (\n",
    "                    repo[1],\n",
    "                    repo[0],\n",
    "                    f\"after: \\\"{cursor}\\\"\" if cursor else \"\" )\n",
    "                  }\n",
    "                ) as resp: \n",
    "              commit_query_response = await resp.json()\n",
    "    except(asyncio.exceptions.TimeoutError,\n",
    "            aiohttp.client_exceptions.ClientConnectorError,\n",
    "            aiohttp.client_exceptions.ServerDisconnectedError\n",
    "          ) as e:\n",
    "      print(f\"A connection error occurred on page {page} of {repo[0]}/{repo[1]}: {e}\")\n",
    "      continue\n",
    "      \n",
    "    # If we hit the secondary rate limit we wait for a second as suggested by the documentation\n",
    "    if resp.status == 403 and commit_query_response.get(\"message\") == seconday_rate_limit_message:\n",
    "      wait = time.time() + 1\n",
    "      continue\n",
    "    \n",
    "    # If we encounter an error we write it to the error log\n",
    "    # Each repo gets its own file, in order to prevent two threads from writing to the same file at the same time\n",
    "    # as well as to identify repos which we have reacurring errors with\n",
    "    \n",
    "    if  commit_query_response.get(\"errors\") or resp.status != 200:\n",
    "        print(\"ERROR!\")\n",
    "        with open(f\"{config['GENERAL']['ERROR_LOG']}/{repo[1]}_{repo[0]}.log\", \"a+\") as error_log:\n",
    "          error_log.write(f\"[{datetime.now()}] : {resp.status} : {commit_query_response} : {resp.headers} \\n \")\n",
    "        return; \n",
    "      \n",
    "    # If we are close to the rate limit we sleep until the rate limit resets\n",
    "    if commit_query_response[\"data\"][\"rateLimit\"][\"remaining\"] <= 10:\n",
    "      wait = datetime.strptime(commit_query_response[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ').timestamp()\n",
    "      continue\n",
    "\n",
    "    # In this part we update the cursor and inform the loop if there are more pages to query\n",
    "    cursor = commit_query_response[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"pageInfo\"][\"endCursor\"]\n",
    "    has_next_page = commit_query_response[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"pageInfo\"][\"hasNextPage\"]\n",
    "    \n",
    "    # This part is where we write the commits to the database\n",
    "    print(f\"Writing commits for page {page} of {repo[0]}/{repo[1]} to database\")\n",
    "    for commit in commit_query_response[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"edges\"]:\n",
    "      commit = commit[\"node\"]\n",
    "      if commit.get(\"author\") and commit[\"author\"].get(\"user\"): \n",
    "        db.execute(INSERT_USER % (\n",
    "                        \", createdAt\" if commit[\"author\"][\"user\"].get(\"createdAt\") else \"\",\n",
    "                        \", location\" if commit[\"author\"][\"user\"].get(\"location\") else \"\",\n",
    "                        commit[\"author\"][\"user\"][\"id\"],\n",
    "                        commit[\"author\"][\"user\"].get(\"login\") if commit[\"author\"][\"user\"].get(\"login\") else \"\",\n",
    "                        f\", \\n {datetime.strptime(commit['author']['user']['createdAt'], '%Y-%m-%dT%H:%M:%SZ').timestamp()}\" if commit[\"author\"]['user'].get(\"createdAt\") else \"\",\n",
    "                        \", \\n '%s'\" % (commit['author']['user']['location'].replace(\"'\", r\"''\")) if commit[\"author\"]['user'].get(\"location\") else \"\",\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "      db.execute(INSERT_COMMIT % (\n",
    "                            commit[\"id\"],\n",
    "                            commit_query_response[\"data\"][\"repository\"][\"id\"],\n",
    "                            commit[\"author\"][\"user\"][\"id\"] if commit.get(\"author\") and commit[\"author\"].get(\"user\") else \"\",\n",
    "                            datetime.strptime(commit[\"committedDate\"], '%Y-%m-%dT%H:%M:%SZ').timestamp(),\n",
    "                            commit[\"additions\"],\n",
    "                            commit[\"deletions\"]\n",
    "                        )\n",
    "        )\n",
    "     \n",
    "    # If we are done with one page of the repo we update the database \n",
    "    # and indicate that we are done with that page\n",
    "    page += 1\n",
    "    db.commit()\n",
    "\n",
    "  # If we are done with all pages of the repo we set the allCommits flag to true for that repo\n",
    "  db.execute(f\"\"\" UPDATE repos\n",
    "                  SET allCommits = True\n",
    "                  WHERE id = \"{\n",
    "                    commit_query_response[\"data\"][\"repository\"][\"id\"]\n",
    "                    }\"\n",
    "              \"\"\")\n",
    "  db.commit()\n",
    "  print(f\"Completed {repo[0]}/{repo[1]}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching the Queries\n",
    "\n",
    "Now that we have a way to query the commits, we can start getting the repositories from the database and start querying the commits for them.\\\n",
    "Even tho we want to query many at a time, we still want to keep the loading times of the website low, so we are going to batch the queries.\n",
    "\n",
    "This is done by creating a list of repositories and then querying the commits for them in batches of 100.\\\n",
    "We then wait for these to finish completely and then start the next batch. \\\n",
    "\n",
    "This way we can query the commits for all the repositories in a reasonable amount of time.\\\n",
    "But considering that we have 1.236.664 repositories, this would still take a lot of time ... \n",
    "\n",
    "In case we do not make it in time, we still need to have data which can be used for the analysis.\n",
    "If we where to chronological query the repositories, we would only be able to analyize up to the last queried repository, which would be a shame. \\ \n",
    "But if we query the repositories in a random order, and do not finish in time, we would have data spread out over the entire time range, \\\n",
    "which would mean that we would still be able to analyze the data only having traded some accuracy for a wider spread.\n",
    "\n",
    "> Update: It did indeed not finish in time, but we still got 23 million commits which should be more than enough for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batchsize = 100\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "while True:\n",
    "    df = pd.read_sql_query(\"\"\"\n",
    "                           SELECT users.id, repos.url\n",
    "                           FROM repos\n",
    "                           JOIN users ON repos.owner = users.id\n",
    "                           WHERE repos.allCommits is False\n",
    "                           ORDER BY RANDOM()  \n",
    "                           LIMIT %d\n",
    "                           \"\"\" % batchsize, db)\n",
    "    if df.empty:\n",
    "        break\n",
    "    repos = df[\"url\"].str.split(\"/\").str[3:5].values.tolist()\n",
    "    loop.run_until_complete(\n",
    "        asyncio.gather(\n",
    "            *[get_commits(repo) for repo in repos]\n",
    "        )\n",
    "    )\n",
    "    print (f\"Completed {len(repos)} repos\")\n",
    "print(\"Done!!! :D\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "[1] danvk, “How can I get a list of all public GitHub repos with more than 20 stars?,” Stack Overflow, Feb. 02, 2020. https://stackoverflow.com/q/60022429 (accessed Mar. 07, 2023).\n",
    "\n",
    "\n",
    "[2] D. Vanderkam, “GitHub Stars and the h-index: A Journey,” Medium, Feb. 10, 2020. https://danvdk.medium.com/github-stars-and-the-h-index-a-journey-c104cfe37da6 (accessed Mar. 06, 2023)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
