{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import JSON\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import configparser as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Reads the config file and returns the config object\n",
    "\"\"\"\n",
    "config = cp.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit = requests.get(\n",
    "    config['API']['URL'] +\n",
    "    config['API']['RATE_LIMIT'] ,\n",
    "    auth=(\n",
    "        config['API']['USER'],    \n",
    "        config['API']['KEY']\n",
    "    )   \n",
    ").json()[\"resources\"][\"search\"] \n",
    "\n",
    "json =  requests.get( \n",
    "        config['API']['URL'] +\n",
    "        config['API']['REPOS'],\n",
    "        auth=(\n",
    "            config['API']['USER'],\n",
    "            config['API']['KEY']\n",
    "        )\n",
    "    ).json()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting up the Queries\n",
    "\n",
    "Due to Githubs limitation on the number of 1000 items returned per query\\[1\\] we need to create queries which get less than 1000 items, but still cover the entirety of the dataset.\n",
    "\n",
    "Previous attempts\\[2\\] to solve this exact problem constrained their queries by the amount of stars for each repository.\n",
    "A method, which only works as long a there are less than 1000 repositories with the same amount of stars.\n",
    "\n",
    "This was then mitigated by using the creation date of the repository as a second constraint.\n",
    "As described in their corresponding blog article \\[3\\], this solution works by:\n",
    "\n",
    "* First querying the Github Graphql API to see the result count of how many items a given query would provide\n",
    "* If it is above a count of 1000 results the takes the date of jungest and oldest repository and splits the query in half of the time range\n",
    "* Then the size of these two queries is checked again and if they are still above 1000 results the process is repeated until the size of the queries is below 1000 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to convert a Unix timestamp to a string in the format required by the github api\n",
    "to_string = lambda stamp : datetime.fromtimestamp(stamp).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def split_querys(start, end):\n",
    "  global amount_of_repos\n",
    "  global repos_done\n",
    "\n",
    "  r = requests.post(\n",
    "          'https://api.github.com/graphql',\n",
    "          headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "          json={\"query\": count_query % (to_string(start), to_string(end)),\n",
    "          \"variables\":{}}\n",
    "      )\n",
    "  \n",
    "  # On the first run we get the total number of repos \n",
    "  # This is used to calculate the progress of the script\n",
    "  if (amount_of_repos is None):\n",
    "    amount_of_repos = r.json()[\"data\"][\"search\"][\"repositoryCount\"]\n",
    "    repos_done = 0\n",
    "\n",
    "  # If we are close to the rate limit we sleep until the rate limit resets\n",
    "  if r.json()[\"data\"][\"rateLimit\"][\"remaining\"] < 10:\n",
    "    reset_time = datetime.strptime( r.json()[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    while datetime.now() < reset_time:\n",
    "      seconds_till_reset = (reset_time - datetime.now()).total_seconds()\n",
    "      print (\"Sleeping till %s... %d minutes and %d seconds left...\" % ( reset_time, *divmod(seconds_till_reset, 60)))\n",
    "      time.sleep(5)\n",
    "\n",
    "  # If the number of repos in the repos in the time range is greater than 1000\n",
    "  if r.json()[\"data\"][\"search\"][\"repositoryCount\"] > 1000:\n",
    "    # We split the range in half and do the same query on each half\n",
    "    # This will continue recursively until the number of repos is less than 1000\n",
    "    split_querys(start, (start + end)//2)\n",
    "    split_querys((start + end)//2, end) \n",
    "    \n",
    "  else:\n",
    "    # If we finnaly get a range with less than 1000 repos we add the timestamps to the sections list\n",
    "    sections.append((start, end))\n",
    "    repos_done = repos_done+r.json()[\"data\"][\"search\"][\"repositoryCount\"]\n",
    "    print(f\"Working on {to_string(start)} to {to_string(end)}. Progress: {repos_done/amount_of_repos*100:.2f}%\")\n",
    "# The query to get the number of repos in a given time range as well as the current state of the rate limit\n",
    "count_query = ''' query { \n",
    "                   rateLimit {\n",
    "                    cost\n",
    "                    remaining\n",
    "                    resetAt\n",
    "                  }\n",
    "                  search(\n",
    "                    query:\"is:public, stars:>15, created:%s..%s\"\n",
    "                    type: REPOSITORY, first: 1) {\n",
    "                    repositoryCount\n",
    "                  }\n",
    "                } '''\n",
    "\n",
    "sections = []\n",
    "\n",
    "start = 1167609600 # Timestamp for 2007-01-01 (Github was founded in 2008 so this will cover all repos)\n",
    "end = 1678209714  # Current Time stamp (for consistency will not use time.time()\n",
    "\n",
    "amount_of_repos = None\n",
    "\n",
    "split_querys(start, end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "[1] “Resources in the REST API,” GitHub Docs. https://docs.github.com/en/rest/overview/resources-in-the-rest-api?apiVersion=2022-11-28 (accessed Mar. 07, 2023).\n",
    "\n",
    "[2] danvk, “How can I get a list of all public GitHub repos with more than 20 stars?,” Stack Overflow, Feb. 02, 2020. https://stackoverflow.com/q/60022429 (accessed Mar. 07, 2023).\n",
    "\n",
    "\n",
    "[3] D. Vanderkam, “GitHub Stars and the h-index: A Journey,” Medium, Feb. 10, 2020. https://danvdk.medium.com/github-stars-and-the-h-index-a-journey-c104cfe37da6 (accessed Mar. 06, 2023)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
