{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "* First, we need to import the necessary libraries.\n",
    "* Then we load the configuration file.\n",
    "  * The config file contains information like the database config and the API key\n",
    "* Then we setup the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from IPython.display import JSON\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import configparser as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Reads the config file and returns the config object\n",
    "\"\"\"\n",
    "config = cp.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite\n",
    "\n",
    "I decided to user a SQLite database instead of simply keeping the data in memory or in a JSON file\\nnot just because of the size of the dataset, but also because of the inherent structure of the data itself.\n",
    "\n",
    "As it is to be expected that the the n to n relationship between the Repos and the Contributors will be queried a lot, so it makes more sense to have a database which can handle relational requests, instead of manually joining dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(config['DB']['NAME'])\n",
    "\n",
    "if True: # Set to True to reset the database and start from scratch\n",
    "    db.execute(\"DROP TABLE IF EXISTS users\")\n",
    "    db.execute(\"DROP TABLE IF EXISTS repos\")\n",
    "    db.execute(\"DROP TABLE IF EXISTS contributions\")\n",
    "\n",
    "db.execute(\"\"\"\n",
    "           CREATE TABLE IF NOT EXISTS users (\n",
    "               id TEXT PRIMARY KEY,\n",
    "               name TEXT NOT NULL,\n",
    "               location TEXT,\n",
    "               createdAt datetime\n",
    "            )\n",
    "              \"\"\")\n",
    "\n",
    "db.execute(\"\"\" \n",
    "           CREATE TABLE IF NOT EXISTS repos (\n",
    "               id TEXT PRIMARY KEY,\n",
    "               name TEXT NOT NULL,\n",
    "               owner TEXT,\n",
    "               url TEXT,\n",
    "               stargazerCount INTEGER,\n",
    "               watchers INTEGER,\n",
    "               primaryLanguage text,\n",
    "               isFork boolean,\n",
    "               forkCount INTEGER,\n",
    "               updatedAt datetime,\n",
    "               createdAt datetime,\n",
    "               FOREIGN KEY (owner) REFERENCES users(id)\n",
    "            )\n",
    "           \"\"\")\n",
    "\n",
    "db.execute(\"\"\"\n",
    "           CREATE TABLE IF NOT EXISTS commits (\n",
    "               id TEXT PRIMARY KEY,\n",
    "               createdAt datetime,\n",
    "               additions INTEGER,\n",
    "               deletions INTEGER,\n",
    "               repo TEXT,\n",
    "               user TEXT,\n",
    "               FOREIGN KEY (repo) REFERENCES repos(id),\n",
    "               FOREIGN KEY (user) REFERENCES users(id)\n",
    "            )\n",
    "            \"\"\")\n",
    "db.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting up the Queries\n",
    "\n",
    "Due to Githubs limitation on the number of 1000 items returned per query\\[1\\] we need to create queries which get less than 1000 items, but still cover the entirety of the dataset.\n",
    "\n",
    "Previous attempts\\[2\\] to solve this exact problem constrained their queries by the amount of stars for each repository.\n",
    "A method, which only works as long a there are less than 1000 repositories with the same amount of stars.\n",
    "\n",
    "This was then mitigated by using the creation date of the repository as a second constraint.\n",
    "As described in their corresponding blog article \\[3\\], this solution works by:\n",
    "\n",
    "* First querying the Github Graphql API to see the result count of how many items a given query would provide\n",
    "* If it is above a count of 1000 results the takes the date of jungest and oldest repository and splits the query in half of the time range\n",
    "* Then the size of these two queries is checked again and if they are still above 1000 results the process is repeated until the size of the queries is below 1000 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to convert a Unix timestamp to a string in the format required by the github api\n",
    "to_string = lambda stamp : datetime.fromtimestamp(stamp).strftime('%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_querys(start, end):\n",
    "  global amount_of_repos\n",
    "  global repos_done\n",
    "\n",
    "  repo_count_response = requests.post(\n",
    "          'https://api.github.com/graphql',\n",
    "          headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "          json={\"query\": count_query % (config['GENERAL']['STARS'], to_string(start), to_string(end))}\n",
    "      )\n",
    "  \n",
    "  # On the first run we get the total number of repos \n",
    "  # This is used to calculate the progress of the script\n",
    "  if (amount_of_repos is None):\n",
    "    amount_of_repos = repo_count_response.json()[\"data\"][\"search\"][\"repositoryCount\"]\n",
    "    repos_done = 0\n",
    "\n",
    "  # If we are close to the rate limit we sleep until the rate limit resets\n",
    "  if repo_count_response.json()[\"data\"][\"rateLimit\"][\"remaining\"] < 10:\n",
    "    reset_time = datetime.strptime( repo_count_response.json()[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    while datetime.now() < reset_time:\n",
    "      seconds_till_reset = (reset_time - datetime.now()).total_seconds()\n",
    "      print (\"Sleeping till %s... %d minutes and %d seconds left...\" % ( reset_time, *divmod(seconds_till_reset, 60)))\n",
    "      time.sleep(5)\n",
    "\n",
    "  # If the number of repos in the repos in the time range is greater than 1000\n",
    "  if repo_count_response.json()[\"data\"][\"search\"][\"repositoryCount\"] > 1000:\n",
    "    # We split the range in half and do the same query on each half\n",
    "    # This will continue recursively until the number of repos is less than 1000\n",
    "    split_querys(start, (start + end)//2)\n",
    "    split_querys((start + end)//2, end) \n",
    "    \n",
    "  else:\n",
    "    # If we finnaly get a range with less than 1000 repos we add the timestamps to the sections list\n",
    "    sections.append((start, end))\n",
    "    repos_done = repos_done+repo_count_response.json()[\"data\"][\"search\"][\"repositoryCount\"]\n",
    "    print(f\"Working on {to_string(start)} to {to_string(end)}. Progress: {repos_done/amount_of_repos*100:.2f}%\")\n",
    "    \n",
    "# The query to get the number of repos in a given time range as well as the current state of the rate limit\n",
    "count_query = ''' query { \n",
    "                   rateLimit {\n",
    "                    cost\n",
    "                    remaining\n",
    "                    resetAt\n",
    "                  }\n",
    "                  search(\n",
    "                    query:\"is:public, stars:>%s, created:%s..%s\"\n",
    "                    type: REPOSITORY, first: 1) {\n",
    "                    repositoryCount\n",
    "                  }\n",
    "                } '''\n",
    "\n",
    "sections = []\n",
    "\n",
    "start = 1167609600 # Timestamp for 2007-01-01 (Github was founded in 2008 so this will cover all repos)\n",
    "end = 1678209714  # Current Time stamp (for consistency will not use time.time()\n",
    "\n",
    "amount_of_repos = None\n",
    "\n",
    "split_querys(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sections, open(\"overnight_sections.p\", \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Github API\n",
    "\n",
    "Now that we have can query the github with sizeable chunks of data, we can start to query the API.\n",
    "We are still using the graphql API for this, as it enables us to fetch only the data we actually need.\n",
    "The REST API would require us to fetch the entire repository object, which contains a lot of unnecessary and redundant data.\n",
    "\n",
    "Just because we are now able to query the bite sized chunks of data, doesn't mean that the query will actually return them.\n",
    "In order to keep the loading times of the website low, Github uses pagination to limit the amount of data returned per query.\n",
    "This means that we can only get 100 items per query, which is why we need to use the `endCursor` to get the next 100 items.\n",
    "\n",
    "The cursor functions like a little bookmark, which tells the API where we left off and where to continue from, it needs to be passed as a parameter to the next query.\n",
    "\n",
    "### Querying the Repos\n",
    "\n",
    "The query itself consist of 4 parts:\n",
    "\n",
    "1. A little snippet, requesting the current state of the rate limit, so we can keep track of how many requests we have left and when to stop\n",
    "2. The filter for the repositories consisting of the following:\n",
    "    * Only repositories which are public (this is a bit redundant, as the API only returns public repositories or the ones you have access to)\n",
    "    * A limit on the amount of stars the repository has, everything below 15 is being ignored as it indicates little relevance\n",
    "    * The date range of the repositories, this is where we plug in our previously calculated date ranges\n",
    "3. Then we request a little bit more metadata about the query itself, like the total count of items and the cursor for the next page and whether there is a next page at all\n",
    "4. Then we tell the API exactly what kind of values we are interested in\n",
    "   1. This being information about the repository itself, like the name, the url, the description, the creation date, the amount of stars and the amount of forks\n",
    "   2. but also information about its creator, like the name, the profile creation date and its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_downloaded = 0 # Used to keep track of the progress of the script\n",
    "nodes = [] # Used to store the repos before they are written to the database\n",
    "\n",
    "\n",
    "def download_repos (start, end):\n",
    "  global repos_downloaded \n",
    "  global nodes\n",
    "  cursor = None # Used to keep track of the current page in the query\n",
    "  has_next_page = True # Used to indicate if there are more pages to query\n",
    "\n",
    "  repo_query= \"\"\"\n",
    "                {\n",
    "                  rateLimit {\n",
    "                    cost\n",
    "                    remaining\n",
    "                    resetAt\n",
    "                  }\n",
    "                  search(\n",
    "                    query: \"is:public, stars:>%s, created:%s..%s\"\n",
    "                    %s\n",
    "                    type: REPOSITORY\n",
    "                    first: 100\n",
    "                  ) {\n",
    "                    repositoryCount\n",
    "                    pageInfo {\n",
    "                      hasNextPage\n",
    "                      endCursor\n",
    "                    }\n",
    "                    edges {\n",
    "                      node {\n",
    "                        ... on Repository {\n",
    "                          createdAt\n",
    "                          forkCount\n",
    "                          isFork\n",
    "                          updatedAt\n",
    "                          primaryLanguage {\n",
    "                            name\n",
    "                          }\n",
    "                          watchers {\n",
    "                            totalCount\n",
    "                          }\n",
    "                          stargazerCount\n",
    "                          databaseId\n",
    "                          owner {\n",
    "                            id\n",
    "                            ... on User {\n",
    "                              id\n",
    "                              createdAt\n",
    "                              location\n",
    "                              databaseId\n",
    "                              login\n",
    "                            }\n",
    "                          }\n",
    "                          id\n",
    "                          name\n",
    "                          url\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\"\"\"\n",
    "\n",
    "  while (has_next_page):\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    repo_query_response = requests.post(\n",
    "              'https://api.github.com/graphql',\n",
    "              headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "              json={\"query\": repo_query % (config['GENERAL'][\"STARS\"], to_string(start), to_string(end), f\"after: \\\"{cursor}\\\"\" if cursor else \"\" )}\n",
    "          )\n",
    "    # If we are close to the rate limit we sleep until the rate limit resets\n",
    "    if repo_query_response.json()[\"data\"][\"rateLimit\"][\"remaining\"] < 10:\n",
    "      reset_time = datetime.strptime( repo_query_response.json()[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "      while datetime.now() < reset_time:\n",
    "        seconds_till_reset = (reset_time - datetime.now()).total_seconds()\n",
    "        print (\"Sleeping till %s... %d minutes and %d seconds left...\" % ( reset_time, *divmod(seconds_till_reset, 60)))\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Summing up the progress made so far\n",
    "    repos_downloaded = repos_downloaded + len(repo_query_response.json()[\"data\"][\"search\"][\"edges\"])\n",
    "    \n",
    "    # Updating the cursor and has_next_page variables to know if and where to continue the query\n",
    "    cursor = repo_query_response.json()[\"data\"][\"search\"][\"pageInfo\"][\"endCursor\"]\n",
    "    has_next_page = repo_query_response.json()[\"data\"][\"search\"][\"pageInfo\"][\"hasNextPage\"]\n",
    "    \n",
    "    # Adding the repos to the nodes list\n",
    "    nodes = nodes + repo_query_response.json()[\"data\"][\"search\"][\"edges\"]\n",
    "    \n",
    "    # Presenting the progress of the script\n",
    "    print(f\"\"\"Downloading {repos_downloaded}/{amount_of_repos} repos \\\n",
    "          Requests left: {repo_query_response.json()['data']['rateLimit']['remaining']} \\\n",
    "          Progress: {repos_downloaded/amount_of_repos*100:.2f}%.\"\"\")\n",
    "\n",
    "# Actually calling the previously defined function for each section\n",
    "for section in sections:\n",
    "  download_repos(section[0], section[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nodes, open(\"overnight_nodes.p\", \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the data to the database\n",
    "\n",
    "Now that we collected the data about the repositories, we need to add it to the database.\n",
    "This is done by adding the creator of every repository to the database and then adding the repository itself to the database with a reference to its creator.\n",
    "\n",
    "It would actually be more effiecient to do this step, to better utilize the time in between the queries and would get rid of the need to keep this data in memory.\n",
    "But this would make the code even less readable than it already is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(nodes):\n",
    "    item = item[\"node\"]\n",
    "    \n",
    "    print(f\" Progress: {i/len(nodes)*100:.2f}% Current Repo: {item['name']}\")\n",
    "    \n",
    "    create_user_query = \"\"\"\n",
    "                            INSERT OR IGNORE INTO users(id, name %s %s)\n",
    "                            VALUES( \"%s\", \"%s\" %s %s\n",
    "                            )\n",
    "                        \"\"\" % (\n",
    "                            \", createdAt\" if item[\"owner\"].get(\"createdAt\") else \"\",\n",
    "                            \", location\" if item[\"owner\"].get(\"location\") else \"\",\n",
    "                            item[\"owner\"][\"id\"],\n",
    "                            item[\"owner\"].get(\"login\") if item[\"owner\"].get(\"login\") else \"\",\n",
    "                            f\", \\n {datetime.strptime(item['owner']['createdAt'], '%Y-%m-%dT%H:%M:%SZ').timestamp()}\" if item[\"owner\"].get(\"createdAt\") else \"\",\n",
    "                            f\", \\n \\\"{item['owner']['location']}\\\"\" if item[\"owner\"].get(\"location\") else \"\",\n",
    "                            )\n",
    "    \n",
    "\n",
    "    create_repo_query = \"\"\"\n",
    "                            INSERT OR IGNORE INTO repos(\n",
    "                                id,\n",
    "                                name,\n",
    "                                owner,\n",
    "                                url,\n",
    "                                stargazerCount,\n",
    "                                watchers,\n",
    "                                %s\n",
    "                                isFork,\n",
    "                                forkCount,\n",
    "                                updatedAt,\n",
    "                                createdAt\n",
    "                            )\n",
    "                            VALUES(\"%s\", \"%s\", \"%s\",\"%s\",  %s,  %s, %s %s, %s, %s, %s)\n",
    "                        \"\"\" % (\n",
    "                                f\"primaryLanguage,\" if item.get(\"primaryLanguage\") else \"\",\n",
    "                                item[\"id\"],\n",
    "                                item[\"name\"],\n",
    "                                item[\"owner\"][\"id\"],\n",
    "                                item[\"url\"],\n",
    "                                item[\"stargazerCount\"],\n",
    "                                item[\"watchers\"][\"totalCount\"],\n",
    "                                f'\"{item[\"primaryLanguage\"][\"name\"]}\",' if item.get(\"primaryLanguage\") else \"\",\n",
    "                                item[\"isFork\"],\n",
    "                                item[\"forkCount\"],\n",
    "                                datetime.strptime(item[\"updatedAt\"], '%Y-%m-%dT%H:%M:%SZ').timestamp(),\n",
    "                                datetime.strptime(item[\"createdAt\"], '%Y-%m-%dT%H:%M:%SZ').timestamp()\n",
    "                            )\n",
    "    db.execute(create_user_query)\n",
    "    db.execute(create_repo_query)\n",
    "\n",
    "    db.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries for the Commits\n",
    "\n",
    "But we do not only want the data about the repositories and the person who iniially created them, we also want to know who actually worked on them.\n",
    "This is where the commits come in.\n",
    "\n",
    "The commits are the actual changes to the code, which are made by the contributors. \n",
    "We could have queried this during the repository query, but this seems to overload the API and would make the code utterly unreadable.\n",
    "\n",
    "This is why we simply query the commits for every repository separately, which makes the query itself a lot simpler.\n",
    "\n",
    "To get to the commits we need to use the creator name and the repository name, as the API does not provide a unique identifier for the repositories.\n",
    "But we cannot simply use the creator name we have in the database, as the names of Organizations are not counted as actual names...\n",
    "\n",
    "To get to the data anyway, we simply take the name provided within the repository URL and use that as the creator name."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_sql_query(\"\"\"\n",
    "                       SELECT users.id, repos.url\n",
    "                       FROM repos\n",
    "                       join users on repos.owner = users.id\n",
    "                       \"\"\", db)\n",
    "owner_name_list = df[\"url\"].str.split(\"/\").str[3:5].values.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for item in owner_name_list:\n",
    "    cursor = None # Used to keep track of the current page in the query\n",
    "    has_next_page = True # Used to indicate if there are more pages to query\n",
    "    commits_total = None # Used to keep track of the total number of commits\n",
    "    commits_downloaded = 0\n",
    "    commit_query = \"\"\"\n",
    "                     {\n",
    "                      rateLimit {\n",
    "                        cost\n",
    "                        remaining\n",
    "                        resetAt\n",
    "                      }\n",
    "                      repository(name: \"%s\", owner: \"%s\") {\n",
    "                        defaultBranchRef {\n",
    "                          target {\n",
    "                            ... on Commit {\n",
    "                              id\n",
    "                              history(first: 100 %s) {\n",
    "                                edges {\n",
    "                                  node {\n",
    "                                    id\n",
    "                                    committedDate\n",
    "                                    additions\n",
    "                                    deletions   \n",
    "                                    author {\n",
    "                                      user {\n",
    "                                        id\n",
    "                                        login\n",
    "                                        location\n",
    "                                      }\n",
    "                                    }\n",
    "                                  }\n",
    "                                }\n",
    "                                totalCount\n",
    "                                pageInfo {\n",
    "                                  endCursor\n",
    "                                  hasNextPage\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\"\"\"\n",
    "\n",
    "    while (has_next_page):\n",
    "        commit_query_response = requests.post(\n",
    "                  'https://api.github.com/graphql',\n",
    "                  headers={'Authorization': 'bearer '+ config['API']['KEY']},\n",
    "                  json={\"query\": commit_query % (item[1], item[0] , f\"after: \\\"{cursor}\\\"\" if cursor else \"\" )}\n",
    "              )\n",
    "        if commit_query_response.json()[\"data\"][\"rateLimit\"][\"remaining\"] <= 10:\n",
    "            reset_time = datetime.strptime(commit_query_response.json()[\"data\"][\"rateLimit\"][\"resetAt\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            while datetime.now() < reset_time:\n",
    "                seconds_till_reset = (reset_time - datetime.now()).total_seconds()\n",
    "                print (\"Sleeping till %s... %d minutes and %d seconds left...\" % ( reset_time, *divmod(seconds_till_reset, 60)))\n",
    "                time.sleep(5)\n",
    "\n",
    "        \n",
    "        if commits_total == None:\n",
    "            commits_total = commit_query_response.json()[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"totalCount\"]    \n",
    "        commits_downloaded = commits_downloaded + len(commit_query_response.json()[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"edges\"])\n",
    "\n",
    "        cursor = commit_query_response.json()[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"pageInfo\"][\"endCursor\"]\n",
    "        has_next_page = commit_query_response.json()[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"pageInfo\"][\"hasNextPage\"]\n",
    "        \n",
    "        print(f\"Progress: {commits_downloaded/commits_total*100:.2f}% \\t  Requests left: {commit_query_response.json()['data']['rateLimit']['remaining']} \\t Current Repo: {item[1]} \")\n",
    "        print(commit_query_response.json()[\"data\"][\"repository\"][\"defaultBranchRef\"][\"target\"][\"history\"][\"edges\"])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "[1] “Resources in the REST API,” GitHub Docs. https://docs.github.com/en/rest/overview/resources-in-the-rest-api?apiVersion=2022-11-28 (accessed Mar. 07, 2023).\n",
    "\n",
    "[2] danvk, “How can I get a list of all public GitHub repos with more than 20 stars?,” Stack Overflow, Feb. 02, 2020. https://stackoverflow.com/q/60022429 (accessed Mar. 07, 2023).\n",
    "\n",
    "\n",
    "[3] D. Vanderkam, “GitHub Stars and the h-index: A Journey,” Medium, Feb. 10, 2020. https://danvdk.medium.com/github-stars-and-the-h-index-a-journey-c104cfe37da6 (accessed Mar. 06, 2023)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
